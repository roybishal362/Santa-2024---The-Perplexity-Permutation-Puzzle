# ğŸ„ Santa 2024 Kaggle Competition â€“ Ultra-Optimized Perplexity Minimization ğŸš€

## **ğŸ“Œ Project Overview**
This repository contains my **Kaggle Santa 2024 competition** solution, which focuses on reordering words in jumbled Christmas stories to minimize **perplexity**. The project implements an **advanced optimization pipeline** using **Large Language Models (LLMs), sentence embeddings, and simulated annealing-based search techniques**.  

## **ğŸ† Challenge Objective**
- The input dataset contains **shuffled text passages**.  
- The goal is to **reorder words correctly** while preserving all original words.  
- The **evaluation metric** is **average perplexity** (lower scores are better).  

The competition uses **Gemma 2 9B** for final perplexity scoring.

---

## **ğŸ“‚ Dataset & Evaluation**
- **`sample_submission.csv`**: Contains the dataset with shuffled text.  
- **Evaluation Metric**: **Perplexity** (lower = better readability & coherence).  
- **Final Score Computation**:  
  - Each row's text is **processed and reordered**  
  - **Gemma 2 9B** computes **perplexity per row**  
  - The final submission is **ranked by mean perplexity**  

---

## **ğŸ”§ Implementation Details**
### **1ï¸âƒ£ Model Selection**
To optimize perplexity, I tested multiple **LLMs**:  
- **Gemma 2 9B** (Kaggle Official Model)  
- **Mixtral 8x7B** (Higher efficiency & lower perplexity scores)  

| Model | Perplexity Score â†“ (Lower is Better) |
|------------|----------------|
| **Gemma 2 9B** | **2369.73** |
| **Mixtral 8x7B** | **1833.96** |

ğŸš€ **Mixtral performed significantly better**, reducing perplexity by **535.77 points**.  

---

### **2ï¸âƒ£ Optimization Pipeline**
I developed an **iterative simulated annealing-based approach** to refine word order while maintaining **strict permutation constraints**. The key steps are:

âœ… **Step 1:** Tokenization & LLM Setup  
âœ… **Step 2:** Compute initial perplexity  
âœ… **Step 3:** Iterative word reordering using:  
   - **Word Swapping**  
   - **Block Exchanges**  
   - **Partial Shuffling**  
âœ… **Step 4:** Evaluate **perplexity score** after each iteration  
âœ… **Step 5:** Accept or reject permutations based on **Boltzmann acceptance probability**  
âœ… **Step 6:** Continue iterations until no further improvement  

---

## **ğŸ› ï¸ How to Run**
### **1ï¸âƒ£ Install Dependencies**
```bash
pip install torch transformers sentence-transformers pandas numpy 
```

## **ğŸ” Key Insights & Findings
- Mixtral 8x7B significantly outperforms Gemma 2 in this task.
- Using adaptive temperature annealing (5.0 â†’ 0.5) leads to more refined word order.
- Strict permutation validation ensures rule compliance and prevents data leakage.
- Quantization techniques (4-bit loading) reduce VRAM usage while maintaining performance.



