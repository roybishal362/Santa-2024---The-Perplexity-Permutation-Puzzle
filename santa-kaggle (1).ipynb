{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -q  -U transformers bitsandbytes torch sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# # Replace 'your_token' with your Hugging Face token\n",
    "# login(token=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import transformers\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from typing import List, Dict, Any\n",
    "# import random\n",
    "# import math\n",
    "# import itertools\n",
    "# import gc\n",
    "\n",
    "# # Advanced Configuration\n",
    "# CONFIG = {\n",
    "#     'model_name': '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',  # State-of-the-art model\n",
    "#     'sentence_encoder': 'all-mpnet-base-v2',\n",
    "#     'beam_width': 3,\n",
    "#     'similarity_threshold': 0.8,\n",
    "#     'max_input_length': 1024,\n",
    "#     'device': 'cuda',\n",
    "#     'quantization': {\n",
    "#         'load_in_4bit': True,\n",
    "#         'bnb_4bit_quant_type': \"nf4\",\n",
    "#         'bnb_4bit_compute_dtype': torch.float16,\n",
    "#         'bnb_4bit_use_double_quant': True\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# class AdvancedTextOptimizer:\n",
    "#     def __init__(self, config: Dict[str, Any]):\n",
    "#         self.config = config\n",
    "#         self.device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "#         self.tokenizer = None\n",
    "#         self.model = None\n",
    "#         self.semantic_model = None\n",
    "#         self._load_models()\n",
    "\n",
    "#     def _load_models(self):\n",
    "#         \"\"\"Advanced model loading with memory-efficient strategies\"\"\"\n",
    "#         quantization_config = BitsAndBytesConfig(**self.config['quantization'])\n",
    "        \n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "#             self.config['model_name'], \n",
    "#             device_map='auto',\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "        \n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(\n",
    "#             self.config['model_name'],\n",
    "#             quantization_config=quantization_config,\n",
    "#             device_map='auto',\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "        \n",
    "#         self.semantic_model = SentenceTransformer(\n",
    "#             self.config['sentence_encoder'], \n",
    "#             device=str(self.device)\n",
    "#         )\n",
    "\n",
    "#     def calculate_perplexity(self, text: str) -> float:\n",
    "#         \"\"\"Optimized perplexity calculation\"\"\"\n",
    "#         try:\n",
    "#             inputs = self.tokenizer(\n",
    "#                 text, \n",
    "#                 return_tensors=\"pt\", \n",
    "#                 truncation=True, \n",
    "#                 max_length=self.config['max_input_length']\n",
    "#             ).to(self.device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.model(**inputs)\n",
    "#                 logits = outputs.logits[0, :-1, :]\n",
    "#                 targets = inputs.input_ids[0, 1:]\n",
    "                \n",
    "#                 loss = torch.nn.functional.cross_entropy(\n",
    "#                     logits.view(-1, logits.size(-1)), \n",
    "#                     targets.view(-1)\n",
    "#                 )\n",
    "                \n",
    "#             return torch.exp(loss).item()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Perplexity error: {e}\")\n",
    "#             return float('inf')\n",
    "\n",
    "#     def semantic_similarity(self, ref: str, candidates: List[str]) -> List[float]:\n",
    "#         \"\"\"Enhanced semantic similarity\"\"\"\n",
    "#         try:\n",
    "#             ref_emb = self.semantic_model.encode([ref])\n",
    "#             cand_embs = self.semantic_model.encode(candidates)\n",
    "#             similarities = np.dot(cand_embs, ref_emb.T).flatten()\n",
    "#             return similarities.tolist()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Similarity error: {e}\")\n",
    "#             return [0.0] * len(candidates)\n",
    "\n",
    "#     def advanced_simulated_annealing(\n",
    "#         self, \n",
    "#         text: str, \n",
    "#         temp_start=6.0, \n",
    "#         temp_end=1.0, \n",
    "#         cooling_rate=0.15,\n",
    "#         steps_per_temp=5\n",
    "#     ):\n",
    "#         \"\"\"Advanced Simulated Annealing with multiple optimization strategies\"\"\"\n",
    "#         words = text.split()\n",
    "#         current = words[:]\n",
    "#         current_score = self.calculate_perplexity(' '.join(current))\n",
    "\n",
    "#         best = current[:]\n",
    "#         best_score = current_score\n",
    "#         temp = temp_start\n",
    "\n",
    "#         while temp > temp_end:\n",
    "#             for _ in range(steps_per_temp):\n",
    "#                 # Multiple neighborhood generation strategies\n",
    "#                 strategies = [\n",
    "#                     self._swap_strategy,\n",
    "#                     self._block_swap_strategy,\n",
    "#                     self._partial_shuffle_strategy\n",
    "#                 ]\n",
    "#                 strategy = random.choice(strategies)\n",
    "                \n",
    "#                 neighbor = strategy(current)\n",
    "#                 neighbor_score = self.calculate_perplexity(' '.join(neighbor))\n",
    "\n",
    "#                 delta = neighbor_score - current_score\n",
    "#                 acceptance_prob = math.exp(-delta / temp)\n",
    "\n",
    "#                 if delta < 0 or random.random() < acceptance_prob:\n",
    "#                     current, current_score = neighbor, neighbor_score\n",
    "                    \n",
    "#                     if current_score < best_score:\n",
    "#                         best, best_score = current[:], current_score\n",
    "\n",
    "#             temp -= cooling_rate\n",
    "\n",
    "#         return ' '.join(best), best_score\n",
    "\n",
    "#     def _swap_strategy(self, words):\n",
    "#         \"\"\"Simple word swap strategy\"\"\"\n",
    "#         neighbor = words[:]\n",
    "#         i, j = random.sample(range(len(words)), 2)\n",
    "#         neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n",
    "#         return neighbor\n",
    "\n",
    "#     def _block_swap_strategy(self, words):\n",
    "#         \"\"\"Block swap strategy for more complex permutations\"\"\"\n",
    "#         block_size = random.randint(2, min(5, len(words)//2))\n",
    "#         neighbor = words[:]\n",
    "#         start1, start2 = random.sample(range(len(words) - block_size), 2)\n",
    "        \n",
    "#         block1 = neighbor[start1:start1+block_size]\n",
    "#         block2 = neighbor[start2:start2+block_size]\n",
    "        \n",
    "#         neighbor[start1:start1+block_size] = block2\n",
    "#         neighbor[start2:start2+block_size] = block1\n",
    "        \n",
    "#         return neighbor\n",
    "\n",
    "#     def _partial_shuffle_strategy(self, words):\n",
    "#         \"\"\"Partial shuffle strategy for local exploration\"\"\"\n",
    "#         neighbor = words[:]\n",
    "#         shuffle_start = random.randint(0, len(words)//2)\n",
    "#         shuffle_end = random.randint(shuffle_start + 1, len(words))\n",
    "        \n",
    "#         subset = neighbor[shuffle_start:shuffle_end]\n",
    "#         random.shuffle(subset)\n",
    "        \n",
    "#         neighbor[shuffle_start:shuffle_end] = subset\n",
    "#         return neighbor\n",
    "\n",
    "# def optimize_dataset(input_path, output_path):\n",
    "#     \"\"\"Comprehensive dataset optimization pipeline\"\"\"\n",
    "#     optimizer = AdvancedTextOptimizer(CONFIG)\n",
    "    \n",
    "#     # Load dataset\n",
    "#     data = pd.read_csv(input_path)\n",
    "#     results = []\n",
    "\n",
    "#     for index, row in data.iterrows():\n",
    "#         print(f\"Optimizing row {index}\")\n",
    "        \n",
    "#         # Multiple optimization techniques\n",
    "#         optimized_text, score = optimizer.advanced_simulated_annealing(row['text'])\n",
    "        \n",
    "#         results.append({\n",
    "#             'id': row['id'],\n",
    "#             'text': optimized_text,\n",
    "#             'perplexity': score\n",
    "#         })\n",
    "        \n",
    "#         # Memory management\n",
    "#         if index % 5 == 0:\n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#     # Create result DataFrame\n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "#     print(f\"Optimization complete. Results saved to {output_path}\")\n",
    "#     print(f\"Average Perplexity: {results_df['perplexity'].mean()}\")\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_path = \"/kaggle/input/santa-2024/sample_submission.csv\"\n",
    "#     output_path = \"optimized_submission.csv\"\n",
    "#     optimize_dataset(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIXTRIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T09:01:05.296978Z",
     "iopub.status.busy": "2025-01-25T09:01:05.296745Z",
     "iopub.status.idle": "2025-01-25T09:36:01.718240Z",
     "shell.execute_reply": "2025-01-25T09:36:01.717276Z",
     "shell.execute_reply.started": "2025-01-25T09:01:05.296955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import transformers\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from typing import List, Dict, Any\n",
    "# import random\n",
    "# import math\n",
    "# import gc\n",
    "\n",
    "# # Advanced Configuration\n",
    "# CONFIG = {\n",
    "#     'model_name': '/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1',  # State-of-the-art model\n",
    "#     'sentence_encoder': 'all-mpnet-base-v2',\n",
    "#     'beam_width': 5,\n",
    "#     'similarity_threshold': 0.85,\n",
    "#     'max_input_length': 2048,\n",
    "#     'device': 'cuda',\n",
    "#     'quantization': {\n",
    "#         'load_in_4bit': True,\n",
    "#         'bnb_4bit_quant_type': \"nf4\",\n",
    "#         'bnb_4bit_compute_dtype': torch.float16,\n",
    "#         'bnb_4bit_use_double_quant': True\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# class AdvancedTextOptimizer:\n",
    "#     def __init__(self, config: Dict[str, Any]):\n",
    "#         self.config = config\n",
    "#         self.device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "#         self.tokenizer = None\n",
    "#         self.model = None\n",
    "#         self.semantic_model = None\n",
    "#         self._load_models()\n",
    "\n",
    "#     def _load_models(self):\n",
    "#         \"\"\"Advanced model loading with memory-efficient strategies\"\"\"\n",
    "#         quantization_config = BitsAndBytesConfig(**self.config['quantization'])\n",
    "        \n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "#             self.config['model_name'], \n",
    "#             device_map='auto',\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "        \n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(\n",
    "#             self.config['model_name'],\n",
    "#             quantization_config=quantization_config,\n",
    "#             device_map='auto',\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "        \n",
    "#         self.semantic_model = SentenceTransformer(\n",
    "#             self.config['sentence_encoder'], \n",
    "#             device=str(self.device)\n",
    "#         )\n",
    "\n",
    "#     def calculate_perplexity(self, text: str) -> float:\n",
    "#         \"\"\"Optimized perplexity calculation\"\"\"\n",
    "#         try:\n",
    "#             inputs = self.tokenizer(\n",
    "#                 text, \n",
    "#                 return_tensors=\"pt\", \n",
    "#                 truncation=True, \n",
    "#                 max_length=self.config['max_input_length']\n",
    "#             ).to(self.device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.model(**inputs)\n",
    "#                 logits = outputs.logits[0, :-1, :]\n",
    "#                 targets = inputs.input_ids[0, 1:]\n",
    "                \n",
    "#                 loss = torch.nn.functional.cross_entropy(\n",
    "#                     logits.view(-1, logits.size(-1)), \n",
    "#                     targets.view(-1)\n",
    "#                 )\n",
    "                \n",
    "#             return torch.exp(loss).item()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Perplexity error: {e}\")\n",
    "#             return float('inf')\n",
    "\n",
    "#     def validate_permutation(self, original: List[str], candidate: List[str]) -> bool:\n",
    "#         \"\"\"Ensures candidate is a valid permutation of the original\"\"\"\n",
    "#         return sorted(original) == sorted(candidate)\n",
    "\n",
    "#     def advanced_simulated_annealing(\n",
    "#         self, \n",
    "#         text: str, \n",
    "#         temp_start=6.0, \n",
    "#         temp_end=1.0, \n",
    "#         cooling_rate=0.15,\n",
    "#         steps_per_temp=5\n",
    "#     ):\n",
    "#         \"\"\"Advanced Simulated Annealing with strict validation\"\"\"\n",
    "#         words = text.split()\n",
    "#         current = words[:]\n",
    "#         current_score = self.calculate_perplexity(' '.join(current))\n",
    "\n",
    "#         best = current[:]\n",
    "#         best_score = current_score\n",
    "#         temp = temp_start\n",
    "\n",
    "#         while temp > temp_end:\n",
    "#             for _ in range(steps_per_temp):\n",
    "#                 # Multiple neighborhood generation strategies\n",
    "#                 strategies = [\n",
    "#                     self._swap_strategy,\n",
    "#                     self._block_swap_strategy,\n",
    "#                     self._partial_shuffle_strategy\n",
    "#                 ]\n",
    "#                 strategy = random.choice(strategies)\n",
    "                \n",
    "#                 neighbor = strategy(current)\n",
    "\n",
    "#                 # Validate the candidate as a strict permutation\n",
    "#                 if not self.validate_permutation(words, neighbor):\n",
    "#                     continue\n",
    "                \n",
    "#                 neighbor_score = self.calculate_perplexity(' '.join(neighbor))\n",
    "#                 delta = neighbor_score - current_score\n",
    "#                 acceptance_prob = math.exp(-delta / temp)\n",
    "\n",
    "#                 if delta < 0 or random.random() < acceptance_prob:\n",
    "#                     current, current_score = neighbor, neighbor_score\n",
    "                    \n",
    "#                     if current_score < best_score:\n",
    "#                         best, best_score = current[:], current_score\n",
    "\n",
    "#             temp -= cooling_rate\n",
    "\n",
    "#         return ' '.join(best), best_score\n",
    "\n",
    "#     def _swap_strategy(self, words):\n",
    "#         \"\"\"Simple word swap strategy\"\"\"\n",
    "#         neighbor = words[:]\n",
    "#         i, j = random.sample(range(len(words)), 2)\n",
    "#         neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n",
    "#         return neighbor\n",
    "\n",
    "#     def _block_swap_strategy(self, words):\n",
    "#         \"\"\"Block swap strategy for more complex permutations\"\"\"\n",
    "#         block_size = random.randint(2, min(5, len(words)//2))\n",
    "#         neighbor = words[:]\n",
    "#         start1, start2 = random.sample(range(len(words) - block_size), 2)\n",
    "        \n",
    "#         block1 = neighbor[start1:start1+block_size]\n",
    "#         block2 = neighbor[start2:start2+block_size]\n",
    "        \n",
    "#         neighbor[start1:start1+block_size] = block2\n",
    "#         neighbor[start2:start2+block_size] = block1\n",
    "        \n",
    "#         return neighbor\n",
    "\n",
    "#     def _partial_shuffle_strategy(self, words):\n",
    "#         \"\"\"Partial shuffle strategy for local exploration\"\"\"\n",
    "#         neighbor = words[:]\n",
    "#         shuffle_start = random.randint(0, len(words)//2)\n",
    "#         shuffle_end = random.randint(shuffle_start + 1, len(words))\n",
    "        \n",
    "#         subset = neighbor[shuffle_start:shuffle_end]\n",
    "#         random.shuffle(subset)\n",
    "        \n",
    "#         neighbor[shuffle_start:shuffle_end] = subset\n",
    "#         return neighbor\n",
    "\n",
    "# def optimize_dataset(input_path, output_path):\n",
    "#     \"\"\"Comprehensive dataset optimization pipeline\"\"\"\n",
    "#     optimizer = AdvancedTextOptimizer(CONFIG)\n",
    "    \n",
    "#     # Load dataset\n",
    "#     data = pd.read_csv(input_path)\n",
    "#     results = []\n",
    "\n",
    "#     for index, row in data.iterrows():\n",
    "#         print(f\"Optimizing row {index}\")\n",
    "        \n",
    "#         # Multiple optimization techniques\n",
    "#         optimized_text, score = optimizer.advanced_simulated_annealing(row['text'])\n",
    "        \n",
    "#         # Final validation step\n",
    "#         original_words = row['text'].split()\n",
    "#         optimized_words = optimized_text.split()\n",
    "        \n",
    "#         if not optimizer.validate_permutation(original_words, optimized_words):\n",
    "#             print(f\"Row {index} failed validation. Skipping.\")\n",
    "#             continue\n",
    "        \n",
    "#         results.append({\n",
    "#             'id': row['id'],\n",
    "#             'text': optimized_text,\n",
    "#             'perplexity': score\n",
    "#         })\n",
    "        \n",
    "#         # Memory management\n",
    "#         if index % 5 == 0:\n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#     # Create result DataFrame\n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "#     print(f\"Optimization complete. Results saved to {output_path}\")\n",
    "#     print(f\"Average Perplexity: {results_df['perplexity'].mean()}\")\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_path = \"/kaggle/input/santa-2024/sample_submission.csv\"\n",
    "#     output_path = \"submission.csv\"\n",
    "#     optimize_dataset(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtril v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import random\n",
    "import math\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "# Ultra-Advanced Configuration for Mixtral\n",
    "CONFIG = {\n",
    "    'model_path': '/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1',\n",
    "    'sentence_encoder': 'BAAI/bge-large-en-v1.5',\n",
    "    'beam_width': 5,\n",
    "    'similarity_threshold': 0.85,\n",
    "    'max_input_length': 2048,\n",
    "    'device': 'cuda',\n",
    "    'quantization': {\n",
    "        'load_in_4bit': True,\n",
    "        'bnb_4bit_quant_type': \"nf4\",\n",
    "        'bnb_4bit_compute_dtype': torch.bfloat16,\n",
    "        'bnb_4bit_use_double_quant': True\n",
    "    }\n",
    "}\n",
    "\n",
    "class UltraAdvancedTextOptimizer:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "        self._initialize_models()\n",
    "        \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Hyper-optimized model initialization with advanced configurations\"\"\"\n",
    "        quantization_config = BitsAndBytesConfig(**self.config['quantization'])\n",
    "        \n",
    "        model_config = AutoConfig.from_pretrained(self.config['model_path'])\n",
    "        model_config.gradient_checkpointing = True\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config['model_path'], \n",
    "            device_map='auto',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config['model_path'],\n",
    "            quantization_config=quantization_config,\n",
    "            device_map='auto',\n",
    "            config=model_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.semantic_model = SentenceTransformer(\n",
    "            self.config['sentence_encoder'], \n",
    "            device=str(self.device)\n",
    "        )\n",
    "\n",
    "    def is_valid_permutation(self, original: str, permuted: str) -> bool:\n",
    "        \"\"\"Strictly verify if the permuted text is a valid permutation of the original\"\"\"\n",
    "        original_counter = Counter(original.split())\n",
    "        permuted_counter = Counter(permuted.split())\n",
    "        return original_counter == permuted_counter\n",
    "\n",
    "    def calculate_perplexity(self, text: str) -> float:\n",
    "        \"\"\"Hyper-optimized perplexity calculation with robust error handling\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=self.config['max_input_length']\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, labels=inputs.input_ids)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            return torch.exp(loss).item()\n",
    "        except Exception as e:\n",
    "            print(f\"Perplexity Error: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    def advanced_optimization_pipeline(\n",
    "        self, \n",
    "        original_text: str,\n",
    "        max_iterations: int = 50,\n",
    "        temperature_schedule: List[float] = [5.0, 3.0, 1.0, 0.5]\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"Multi-phase optimization with strict permutation validation\"\"\"\n",
    "        words = original_text.split()\n",
    "        current_solution = words[:]\n",
    "        best_solution = words[:]\n",
    "        \n",
    "        current_score = self.calculate_perplexity(' '.join(current_solution))\n",
    "        best_score = current_score\n",
    "\n",
    "        for temp in temperature_schedule:\n",
    "            for _ in range(max_iterations):\n",
    "                # Dynamic neighborhood exploration with permutation validation\n",
    "                valid_solution = False\n",
    "                max_attempts = 5  # Limit attempts to find valid permutation\n",
    "                \n",
    "                for _ in range(max_attempts):\n",
    "                    strategy = random.choice([\n",
    "                        self._swap_words,\n",
    "                        self._block_exchange,\n",
    "                        self._partial_shuffle\n",
    "                    ])\n",
    "                    \n",
    "                    candidate = strategy(current_solution)\n",
    "                    candidate_text = ' '.join(candidate)\n",
    "                    \n",
    "                    if self.is_valid_permutation(original_text, candidate_text):\n",
    "                        valid_solution = True\n",
    "                        break\n",
    "                \n",
    "                if not valid_solution:\n",
    "                    continue\n",
    "                \n",
    "                candidate_score = self.calculate_perplexity(candidate_text)\n",
    "                \n",
    "                # Adaptive acceptance with strict validation\n",
    "                acceptance_prob = math.exp(-(candidate_score - current_score) / temp)\n",
    "                \n",
    "                if candidate_score < current_score or random.random() < acceptance_prob:\n",
    "                    current_solution = candidate\n",
    "                    current_score = candidate_score\n",
    "                    \n",
    "                    if candidate_score < best_score:\n",
    "                        best_solution = candidate\n",
    "                        best_score = candidate_score\n",
    "\n",
    "        final_text = ' '.join(best_solution)\n",
    "        # Final validation check\n",
    "        if not self.is_valid_permutation(original_text, final_text):\n",
    "            return original_text, current_score\n",
    "            \n",
    "        return final_text, best_score\n",
    "\n",
    "    def _swap_words(self, words: List[str]) -> List[str]:\n",
    "        \"\"\"Safe word swapping maintaining permutation validity\"\"\"\n",
    "        if len(words) < 2:\n",
    "            return words\n",
    "        result = words.copy()\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        result[idx1], result[idx2] = result[idx2], result[idx1]\n",
    "        return result\n",
    "\n",
    "    def _block_exchange(self, words: List[str]) -> List[str]:\n",
    "        \"\"\"Safe block exchange maintaining permutation validity\"\"\"\n",
    "        if len(words) < 4:\n",
    "            return words\n",
    "        result = words.copy()\n",
    "        block_size = random.randint(2, max(3, len(words) // 4))\n",
    "        if len(words) - block_size < block_size:\n",
    "            return result\n",
    "            \n",
    "        start1, start2 = random.sample(range(len(words) - block_size), 2)\n",
    "        \n",
    "        block1 = result[start1:start1+block_size]\n",
    "        block2 = result[start2:start2+block_size]\n",
    "        \n",
    "        result[start1:start1+block_size] = block2\n",
    "        result[start2:start2+block_size] = block1\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _partial_shuffle(self, words: List[str]) -> List[str]:\n",
    "        \"\"\"Controlled shuffling maintaining permutation validity\"\"\"\n",
    "        if len(words) < 3:\n",
    "            return words\n",
    "        result = words.copy()\n",
    "        shuffle_size = random.randint(2, max(3, len(words) // 3))\n",
    "        start = random.randint(0, len(words) - shuffle_size)\n",
    "        \n",
    "        subset = result[start:start+shuffle_size]\n",
    "        random.shuffle(subset)\n",
    "        result[start:start+shuffle_size] = subset\n",
    "        \n",
    "        return result\n",
    "\n",
    "def optimize_kaggle_dataset(input_path: str, output_path: str):\n",
    "    \"\"\"Comprehensive Kaggle dataset optimization workflow with validation\"\"\"\n",
    "    optimizer = UltraAdvancedTextOptimizer(CONFIG)\n",
    "    \n",
    "    # Load dataset\n",
    "    data = pd.read_csv(input_path)\n",
    "    results = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        print(f\"Optimizing row {index}\")\n",
    "        \n",
    "        original_text = row['text']\n",
    "        optimized_text, score = optimizer.advanced_optimization_pipeline(original_text)\n",
    "        \n",
    "        # Final validation before saving\n",
    "        if not optimizer.is_valid_permutation(original_text, optimized_text):\n",
    "            print(f\"Warning: Invalid permutation detected for row {index}. Using original text.\")\n",
    "            optimized_text = original_text\n",
    "        \n",
    "        results.append({\n",
    "            'id': row['id'],\n",
    "            'text': optimized_text,\n",
    "            'perplexity': score\n",
    "        })\n",
    "        \n",
    "        # Enhanced memory management\n",
    "        if index % 5 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Optimization complete. Results saved to {output_path}\")\n",
    "    print(f\"Mean Perplexity: {results_df['perplexity'].mean():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"/kaggle/input/santa-2024/sample_submission.csv\"\n",
    "    output_path = \"submission.csv\"\n",
    "    optimize_kaggle_dataset(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new versi9b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T15:48:44.994775Z",
     "iopub.status.busy": "2025-01-27T15:48:44.994480Z",
     "iopub.status.idle": "2025-01-27T15:48:49.215602Z",
     "shell.execute_reply": "2025-01-27T15:48:49.214903Z",
     "shell.execute_reply.started": "2025-01-27T15:48:44.994752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import os\n",
    "# from math import exp\n",
    "# from collections import Counter\n",
    "# from typing import List, Optional, Union\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# class ParticipantVisibleError(Exception):\n",
    "#     pass\n",
    "\n",
    "\n",
    "# def score(\n",
    "#     solution: pd.DataFrame,\n",
    "#     submission: pd.DataFrame,\n",
    "#     row_id_column_name: str,\n",
    "#     model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "#     load_in_8bit: bool = True,\n",
    "#     clear_mem: bool = False,\n",
    "# ) -> float:\n",
    "#     \"\"\"\n",
    "#     Calculates the mean perplexity of submitted text permutations compared to an original text.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     solution : DataFrame\n",
    "#         DataFrame containing the original text in a column named 'text'.\n",
    "#         Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "#     submission : DataFrame\n",
    "#         DataFrame containing the permuted text in a column named 'text'.\n",
    "#         Must have the same row IDs as the solution.\n",
    "#         Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "#     row_id_column_name : str\n",
    "#         Name of the column containing row IDs.\n",
    "#         Ensures aligned comparison between solution and submission.\n",
    "\n",
    "#     model_path : str\n",
    "#         Path to the serialized LLM.\n",
    "\n",
    "#     clear_mem : bool\n",
    "#         Clear GPU memory after scoring by clearing the CUDA cache.\n",
    "#         Useful for testing.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     float\n",
    "#         The mean perplexity score. Lower is better.\n",
    "\n",
    "#     Raises\n",
    "#     ------\n",
    "#     ParticipantVisibleError\n",
    "#         If the submission format is invalid or submitted strings are not valid permutations.\n",
    "\n",
    "#     Examples\n",
    "#     --------\n",
    "#     >>> import pandas as pd\n",
    "#     >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "#     >>> solution = pd.DataFrame({\n",
    "#     ...     'id': [0, 1],\n",
    "#     ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "#     ... })\n",
    "#     >>> submission = pd.DataFrame({\n",
    "#     ...     'id': [0, 1],\n",
    "#     ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n",
    "#     ... })\n",
    "#     >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n",
    "#     True\n",
    "#     \"\"\"\n",
    "#     # Check that each submitted string is a permutation of the solution string\n",
    "#     sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "#     sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "#     invalid_mask = sol_counts != sub_counts\n",
    "#     if invalid_mask.any():\n",
    "#         raise ParticipantVisibleError(\n",
    "#             'At least one submitted string is not a valid permutation of the solution string.'\n",
    "#         )\n",
    "\n",
    "#     # Calculate perplexity for the submitted strings\n",
    "#     sub_strings = [\n",
    "#         ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "#     ]  # Split and rejoin to normalize whitespace\n",
    "#     scorer = PerplexityCalculator(\n",
    "#         model_path=model_path,\n",
    "#         load_in_8bit=load_in_8bit,\n",
    "#     )  # Initialize the perplexity calculator with a pre-trained model\n",
    "#     perplexities = scorer.get_perplexity(\n",
    "#         sub_strings\n",
    "#     )  # Calculate perplexity for each submitted string\n",
    "\n",
    "#     if clear_mem:\n",
    "#         # Just move on if it fails. Not essential if we have the score.\n",
    "#         try:\n",
    "#             scorer.clear_gpu_memory()\n",
    "#         except:\n",
    "#             print('GPU memory clearing failed.')\n",
    "\n",
    "#     return float(np.mean(perplexities))\n",
    "\n",
    "\n",
    "# class PerplexityCalculator:\n",
    "#     \"\"\"\n",
    "#     Calculates perplexity of text using a pre-trained language model.\n",
    "\n",
    "#     Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     model_path : str\n",
    "#         Path to the pre-trained language model\n",
    "\n",
    "#     load_in_8bit : bool, default=False\n",
    "#         Use 8-bit quantization for the model. Requires CUDA.\n",
    "\n",
    "#     device_map : str, default=\"auto\"\n",
    "#         Device mapping for the model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_path: str,\n",
    "#         load_in_8bit: bool = False,\n",
    "#         device_map: str = 'auto',\n",
    "#     ):\n",
    "#         self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path,padding_side=\"right\")\n",
    "#         # Configure model loading based on quantization setting and device availability\n",
    "#         if load_in_8bit:\n",
    "#             if DEVICE.type != 'cuda':\n",
    "#                 raise ValueError('8-bit quantization requires CUDA device')\n",
    "\n",
    "#             #quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "#             #quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "#             quantization_config = transformers.BitsAndBytesConfig(\n",
    "#                 load_in_4bit = True,\n",
    "#                 bnb_4bit_quant_type = \"fp4\", #fp4 nf4\n",
    "#                 bnb_4bit_use_double_quant = False,\n",
    "#                 bnb_4bit_compute_dtype=torch.float16,\n",
    "#             )\n",
    "\n",
    "#             self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#                 model_path,\n",
    "#                 quantization_config=quantization_config,\n",
    "#                 device_map=device_map,\n",
    "#             )\n",
    "#         else:\n",
    "#             self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#                 model_path,\n",
    "#                 torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "#                 device_map=device_map,\n",
    "#             )\n",
    "\n",
    "#         self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "#         self.model.eval()\n",
    "#         #if not load_in_8bit:\n",
    "#         #    self.model.to(DEVICE)  # Explicitly move the model to the device\n",
    "\n",
    "#     def get_perplexity(\n",
    "#         self, input_texts: Union[str, List[str]], batch_size: 32\n",
    "#     ) -> Union[float, List[float]]:\n",
    "#         \"\"\"\n",
    "#         Calculates the perplexity of given texts.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         input_texts : str or list of str\n",
    "#             A single string or a list of strings.\n",
    "\n",
    "#         batch_size : int, default=None\n",
    "#             Batch size for processing. Defaults to the number of input texts.\n",
    "\n",
    "#         verbose : bool, default=False\n",
    "#             Display progress bar.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         float or list of float\n",
    "#             A single perplexity value if input is a single string,\n",
    "#             or a list of perplexity values if input is a list of strings.\n",
    "\n",
    "#         Examples\n",
    "#         --------\n",
    "#         >>> import pandas as pd\n",
    "#         >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "#         >>> scorer = PerplexityCalculator(model_path=model_path)\n",
    "\n",
    "#         >>> submission = pd.DataFrame({\n",
    "#         ...     'id': [0, 1, 2],\n",
    "#         ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "#         ... })\n",
    "#         >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "#         >>> perplexities[0] < perplexities[1]\n",
    "#         True\n",
    "#         >>> perplexities[2] < perplexities[0]\n",
    "#         True\n",
    "\n",
    "#         >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n",
    "#         >>> all(p > 0 for p in perplexities)\n",
    "#         True\n",
    "\n",
    "#         >>> scorer.clear_gpu_memory()\n",
    "#         \"\"\"\n",
    "#         single_input = isinstance(input_texts, str)\n",
    "#         input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "#         loss_list = []\n",
    "\n",
    "#         batches = len(input_texts)//batch_size + (len(input_texts)%batch_size != 0)\n",
    "#         for j in range(batches):\n",
    "\n",
    "#             a = j*batch_size\n",
    "#             b = (j+1)*batch_size\n",
    "#             input_batch = input_texts[a:b]\n",
    "\n",
    "#             with torch.no_grad():\n",
    "\n",
    "#                 # Explicitly add sequence boundary tokens to the text\n",
    "#                 text_with_special = [f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in input_batch]\n",
    "\n",
    "#                 # Tokenize\n",
    "#                 model_inputs = self.tokenizer(\n",
    "#                     text_with_special,\n",
    "#                     return_tensors='pt',\n",
    "#                     add_special_tokens=False,\n",
    "#                     padding=True\n",
    "#                 )\n",
    "\n",
    "#                 if 'token_type_ids' in model_inputs:\n",
    "#                     model_inputs.pop('token_type_ids')\n",
    "\n",
    "#                 model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "#                 # Get model output\n",
    "#                 output = self.model(**model_inputs, use_cache=False)\n",
    "#                 logits = output['logits']\n",
    "\n",
    "#                 label = model_inputs['input_ids']\n",
    "#                 label[label == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n",
    "\n",
    "#                 # Shift logits and labels for calculating loss\n",
    "#                 shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "#                 shift_labels = label[..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "#                 # Calculate token-wise loss\n",
    "#                 loss = self.loss_fct(\n",
    "#                     shift_logits.view(-1, shift_logits.size(-1)),\n",
    "#                     shift_labels.view(-1)\n",
    "#                 )\n",
    "\n",
    "#                 loss = loss.view(len(logits), -1)\n",
    "#                 valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n",
    "#                 loss = torch.sum(loss, -1) / valid_length\n",
    "\n",
    "#                 loss_list += loss.cpu().tolist()\n",
    "\n",
    "#                 # Debug output\n",
    "#                 #print(f\"\\nProcessing: '{text}'\")\n",
    "#                 #print(f\"With special tokens: '{text_with_special}'\")\n",
    "#                 #print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
    "#                 #print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
    "#                 #print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
    "#                 #print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
    "#                 #print(f\"Individual losses: {loss.tolist()}\")\n",
    "#                 #print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
    "\n",
    "#         ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "#         # print(\"\\nFinal perplexities:\")\n",
    "#         # for text, perp in zip(input_texts, ppl):\n",
    "#         #     print(f\"Text: '{text}'\")\n",
    "#         #     print(f\"Perplexity: {perp:.2f}\")\n",
    "\n",
    "#         return ppl[0] if single_input else ppl\n",
    "\n",
    "#     def clear_gpu_memory(self) -> None:\n",
    "#         \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
    "#         if not torch.cuda.is_available():\n",
    "#             return\n",
    "\n",
    "#         # Delete model and tokenizer if they exist\n",
    "#         if hasattr(self, 'model'):\n",
    "#             del self.model\n",
    "#         if hasattr(self, 'tokenizer'):\n",
    "#             del self.tokenizer\n",
    "\n",
    "#         # Run garbage collection\n",
    "#         gc.collect()\n",
    "\n",
    "#         # Clear CUDA cache and reset memory stats\n",
    "#         with DEVICE:\n",
    "#             torch.cuda.empty_cache()\n",
    "#             torch.cuda.ipc_collect()\n",
    "#             torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T15:49:07.014795Z",
     "iopub.status.busy": "2025-01-27T15:49:07.014499Z",
     "iopub.status.idle": "2025-01-27T15:50:20.263889Z",
     "shell.execute_reply": "2025-01-27T15:50:20.261618Z",
     "shell.execute_reply.started": "2025-01-27T15:49:07.014771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# santa_2024_path=\"/kaggle/input/santa-2024\"\n",
    "\n",
    "# google_gemma_2_transformers_gemma_2_9b_2_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n",
    "\n",
    "# scorer = PerplexityCalculator(google_gemma_2_transformers_gemma_2_9b_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.264517Z",
     "iopub.status.idle": "2025-01-27T15:50:20.264812Z",
     "shell.execute_reply": "2025-01-27T15:50:20.264668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# samples=pd.read_csv(santa_2024_path+\"/sample_submission.csv\")\n",
    "\n",
    "# import numpy as np\n",
    "# BATCH_SIZE = 64\n",
    "\n",
    "# best_score = 1e6\n",
    "# import gc\n",
    "# import os\n",
    "# from math import exp\n",
    "# from collections import Counter\n",
    "# from typing import List, Optional, Union\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import transformers\n",
    "# import torch\n",
    "# import math\n",
    "\n",
    "\n",
    "# from functools import lru_cache\n",
    "\n",
    "# import random\n",
    "# import math\n",
    "# import random, pickle, warnings\n",
    "\n",
    "# samples.loc[0,\"text\"]='reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament'\n",
    "\n",
    "# samples.loc[1,\"text\"]='reindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament'\n",
    "\n",
    "# samples.loc[2,\"text\"]='sleigh yuletide beard carol cheer chimney decorations gifts grinch holiday holly jingle magi naughty nice nutcracker ornament polar workshop stocking'\n",
    "\n",
    "# samples.loc[3,\"text\"]='sleigh of the magi yuletide cheer is unwrap gifts and eat cheer holiday decorations holly jingle relax sing carol visit workshop grinch naughty nice chimney stocking ornament nutcracker polar beard'\n",
    "\n",
    "# samples.loc[4,\"text\"]='from and of to the as in that it we with not you have milk chocolate candy peppermint eggnog cookie fruitcake toy doll game puzzle greeting card wrapping paper bow wreath poinsettia snowglobe candle fireplace wish dream hope believe wonder night star angel peace joy season merry hohoho kaggle workshop'\n",
    "# samples.loc[5,\"text\"]='from and and as we and have the in is it of not that the to with you advent card angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game gifts give gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament of the wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide'\n",
    "\n",
    "# perplexities2 = []\n",
    "\n",
    "\n",
    "# for index, row in samples.iterrows(): # Step 1: Reorder the words based on POS tagging reordered_text = reorder_text(row[\"text\"])\n",
    "#     score=scorer.get_perplexity(row['text'],batch_size=BATCH_SIZE)\n",
    "#     perplexities2.append(score)\n",
    "#     print(f\"i={index}:{score}\")\n",
    "# np.sum(perplexities2)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.265743Z",
     "iopub.status.idle": "2025-01-27T15:50:20.266134Z",
     "shell.execute_reply": "2025-01-27T15:50:20.265952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# def get_best_plex(df,ix, shift=0, rev=False, tf=False):\n",
    "#     print('SHIFT: ', shift, ' Score: ', df.loc[ix,'score'])\n",
    "#     for r in [ix]:\n",
    "#         if rev: shift+=1\n",
    "#         if len(df['text'][r].split(' ')) > shift:\n",
    "#             bGood = True\n",
    "#             while bGood == True:\n",
    "#                 bGood = False\n",
    "#                 t = df['text'][r].split(' ')\n",
    "#                 if rev: shift *= -1\n",
    "#                 last = [t[shift]]\n",
    "#                 del t[shift]\n",
    "#                 t = t + last\n",
    "#                 best = df['score'][r]\n",
    "#                 for x in range(len(t)-1):\n",
    "#                     new = t[:x] + last + t[x:-1]\n",
    "#                     new = ' '.join(new)\n",
    "#                     s = scorer.get_perplexity(new, batch_size=BATCH_SIZE)\n",
    "#                     if s < best:\n",
    "#                         bGood = True\n",
    "#                         best = s\n",
    "#                         df.at[r, 'score'] = s\n",
    "#                         df.at[r, 'text'] = new\n",
    "#                         print(r, x, \"New Score: \", s)\n",
    "#                         print(new)\n",
    "#                         if tf: break #take first shift\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def simulated_annealing_optimize_nb3(text: str, temp_start=6.0, temp_end=1.0, cooling_rate=0.2,\n",
    "#                                      n_neighbor=2, steps_per_temp=4, verbose=False, seq_to_choose=None,\n",
    "#                                      factor_acceptance=3,\n",
    "#                                      shuffle_start: bool = False, shuffle_pos=[], check_komb=True):\n",
    "\n",
    "\n",
    "#     words = text.split()\n",
    "#     current = words[:]\n",
    "#     current_score = scorer.get_perplexity(' '.join(current), batch_size=BATCH_SIZE)\n",
    "#     if check_komb:\n",
    "#         res=komb_words.loc[komb_words[\"text\"]==text]\n",
    "#         if len(res)>0:\n",
    "#             print(f\"Kombination has been looked before\")\n",
    "#             return text,current_score\n",
    "#     while math.isnan(current_score):\n",
    "#         random.shuffle(current)\n",
    "#         current_score = scorer.get_perplexity(' '.join(current), batch_size=BATCH_SIZE)\n",
    "#     print(f\"current={' '.join(current)}: {current_score}\")\n",
    "#     if shuffle_start:\n",
    "#         shuffle_pos = [i for i in shuffle_pos if i < len(current)]\n",
    "#         print(f\"Shuffle positions: {shuffle_pos}\")\n",
    "#         shuffled_values = random.sample([current[i] for i in shuffle_pos], len(shuffle_pos))\n",
    "#         for i, pos in enumerate(shuffle_pos):\n",
    "#             current[pos] = shuffled_values[i]\n",
    "#         current_score = scorer.get_perplexity(' '.join(current), batch_size=BATCH_SIZE)\n",
    "#         print(f\"After shuffle: current={' '.join(current)}: {current_score}\")\n",
    "#     best = current[:]\n",
    "#     best_score = current_score\n",
    "#     temp = temp_start\n",
    "\n",
    "#     seq_no_k = seq_to_choose if seq_to_choose is not None else list(range(len(words)))\n",
    "#     print(f\"Seq to choose from: {seq_no_k}\")\n",
    "#     while temp > temp_end:\n",
    "#         for _ in range(steps_per_temp):\n",
    "\n",
    "#             indices = random.sample(seq_no_k, min(n_neighbor, len(seq_no_k)))\n",
    "#             if n_neighbor == 2:\n",
    "#                 neighbor = current[:]\n",
    "#                 neighbor[indices[0]], neighbor[indices[1]] = neighbor[indices[1]], neighbor[indices[0]]\n",
    "#                 neighbor_score = scorer.get_perplexity(' '.join(neighbor), batch_size=BATCH_SIZE)\n",
    "#             elif n_neighbor==3:\n",
    "#                 neighbor = current[:]\n",
    "#                 neighbor_variants = []\n",
    "#                 for perm in [(1, 0, 2), (2, 0, 1), (0, 2, 1), (2, 1, 0), (1, 2, 0)]:\n",
    "#                     variant = current[:]\n",
    "#                     variant[indices[0]], variant[indices[1]], variant[indices[2]] = \\\n",
    "#                         neighbor[indices[perm[0]]], neighbor[indices[perm[1]]], neighbor[indices[perm[2]]]\n",
    "#                     neighbor_variants.append(' '.join(variant))\n",
    "\n",
    "#                 #perplexities = [scorer.get_perplexity(text, batch_size=BATCH_SIZE) for text in neighbor_variants]\n",
    "#                 perplexities = scorer.get_perplexity(neighbor_variants, batch_size=BATCH_SIZE)\n",
    "#                 best_variant_idx = perplexities.index(min(perplexities))\n",
    "#                 neighbor = neighbor_variants[best_variant_idx].split()\n",
    "#                 neighbor_score = scorer.get_perplexity(' '.join(neighbor), batch_size=BATCH_SIZE)\n",
    "#             else:\n",
    "#                 print(\"n_neigbor must be either 2 or 3\")\n",
    "#                 return 0\n",
    "#             if math.isnan(neighbor_score):\n",
    "#                 continue\n",
    "\n",
    "#             delta = neighbor_score - current_score\n",
    "#             acceptance_prob = math.exp(-delta / temp) * factor_acceptance\n",
    "#             if delta < 0 or random.random() < acceptance_prob:\n",
    "#                 #print(f\"acceptance_prob: {acceptance_prob}\")\n",
    "#                 current, current_score = neighbor[:], neighbor_score\n",
    "#                 #if verbose: print(f\"Temperature: {temp:.2f}, Current score:{current} {current_score:.2f}\")\n",
    "#                 #if verbose: print(f\"Temperature: {temp:.2f}, Neigbor score: {neighbor_score:.2f}\")\n",
    "#                 if current_score < best_score:\n",
    "#                     best, best_score = current[:], current_score\n",
    "#                     if verbose>0:\n",
    "#                         print(f\"New best: {' '.join(best)}\")\n",
    "#                         print(f\"New best score: {best_score:.2f}\")\n",
    "\n",
    "#         temp -= cooling_rate\n",
    "#         if verbose>0:\n",
    "#             print(f\"Temperature: {temp:.2f}, Current: {' '.join(current)}\")\n",
    "#             print(f\"Temperature: {temp:.2f}, Current score: {current_score:.2f}\")\n",
    "\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"Final score: {best_score:.2f}\")\n",
    "#     return ' '.join(best), best_score\n",
    "\n",
    "\n",
    "# def part_perm_brute_lastf(start_word, psize=3):\n",
    "#     \"\"\"\n",
    "#     Efficiently finds the best permutation of the last part of the input string\n",
    "#     to minimize perplexity.\n",
    "#     \"\"\"\n",
    "#     # Initial best word and score\n",
    "#     best_word = start_word\n",
    "#     best_score = scorer.get_perplexity(start_word, batch_size=BATCH_SIZE)\n",
    "#     print(f\"Start score: {best_score}\")\n",
    "\n",
    "#     # Split input into parts\n",
    "#     words = start_word.split(' ')\n",
    "#     rest = ' '.join(words[:psize]) + ' '\n",
    "#     part = words[psize:]\n",
    "\n",
    "#     import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#     # Early exit if there are too many permutations\n",
    "#     if len(part) > 10:\n",
    "#         print(\"Too many permutations to process efficiently.\")\n",
    "#         return start_word\n",
    "\n",
    "\n",
    "\n",
    "#     print(f\"There are {math.factorial(len(part))} permutations.\")\n",
    "#     # Iterate over permutations\n",
    "#     for i, perm in enumerate(itertools.permutations(part)):\n",
    "#         # Create the new word order\n",
    "#         new_word = rest + ' '.join(perm)\n",
    "#         #kombis.loc[i,\"text\"]=new_word\n",
    "\n",
    "#         # Calculate perplexity for the new word order\n",
    "#         new_score = scorer.get_perplexity(new_word, batch_size=BATCH_SIZE)\n",
    "\n",
    "#         #kombis.loc[i,\"score\"]=new_score\n",
    "#         # Log progress every 1000 iterations\n",
    "#         if i % 100 == 0:\n",
    "#             print(f\"Iteration {i}: Current best score = {best_score}\")\n",
    "#             #print(f\"Tested sequence: {new_word} score: {new_score}\")\n",
    "\n",
    "#         # Update best score and word if an improvement is found\n",
    "#         if new_score < best_score:\n",
    "#             print(f\"New best score found: {new_score}\")\n",
    "#             print(f\"New best sequence: {new_word}\")\n",
    "#             best_score = new_score\n",
    "#             best_word = new_word\n",
    "\n",
    "\n",
    "#     return best_word, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.266842Z",
     "iopub.status.idle": "2025-01-27T15:50:20.267240Z",
     "shell.execute_reply": "2025-01-27T15:50:20.267053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ##collection poor starting points\n",
    "\n",
    "# texts = [\n",
    "#     'wreath wish bow doll hope joy peace dream believe wonder star candle night with in and the have of it from as to not you we that poinsettia snowglobe angel fruitcake cookie peppermint candy greeting card fireplace kaggle puzzle game season merry hohoho chocolate milk eggnog toy workshop wrapping paper',\n",
    "#     'merry hohoho the season of eggnog and fruitcake greeting card kaggle workshop from with to in as that we you it have not toy doll puzzle game night chocolate milk cookie peppermint candy candle wreath snowglobe fireplace poinsettia wrapping paper bow angel star wish dream wonder believe hope joy peace'\n",
    "# ]\n",
    "# komb_words=pd.DataFrame({'text':['']*(len(texts)+10), 'score':[1000.0]*(len(texts)+10)})\n",
    "# max_text=len(texts)\n",
    "# for idx, t in enumerate(texts):\n",
    "#     komb_words.loc[idx, \"text\"] = t\n",
    "\n",
    "\n",
    "# for ix in range(len(texts)):\n",
    "#     komb_words.loc[ix,\"score\"]=scorer.get_perplexity(komb_words.loc[ix,\"text\"], batch_size=BATCH_SIZE)\n",
    "# #text1=text\n",
    "# #komb_words.loc[komb_words[\"text\"]==text1]\n",
    "# #komb_words.loc[0,\"score\"]=np.round(score,5)\n",
    "# komb_words.loc[:len(texts),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.268177Z",
     "iopub.status.idle": "2025-01-27T15:50:20.268540Z",
     "shell.execute_reply": "2025-01-27T15:50:20.268380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ##initial solution\n",
    "# txt1=samples.loc[4,\"text\"]\n",
    "# scorer.get_perplexity(txt1,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.269333Z",
     "iopub.status.idle": "2025-01-27T15:50:20.269695Z",
     "shell.execute_reply": "2025-01-27T15:50:20.269533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ## sample 4\n",
    "# ii=4\n",
    "\n",
    "# ## setting seq_to_choose to list(range(21,50)) \n",
    "# #txt1=samples.loc[ii,\"text\"]\n",
    "# temp_text, temp_score=simulated_annealing_optimize_nb3(text=txt1, temp_start=6.0,\n",
    "#                                                     temp_end=1.0, cooling_rate=0.2,\n",
    "#                                 n_neighbor=3,\n",
    "#                                   steps_per_temp=5, verbose=2, seq_to_choose=list(range(22,50)),\n",
    "#                                                            shuffle_start=False,\n",
    "#                                 shuffle_pos=range(21,50), factor_acceptance=0.15,check_komb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.270397Z",
     "iopub.status.idle": "2025-01-27T15:50:20.270744Z",
     "shell.execute_reply": "2025-01-27T15:50:20.270587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ## I get a new good starting point\n",
    "# txt1='from and of to the as in that it we with not you have milk chocolate candy peppermint eggnog cookie fruitcake toy doll puzzle game night season greeting card wrapping paper bow wreath poinsettia candle fireplace snowglobe angel star wish wonder dream believe hope joy peace merry hohoho kaggle workshop'\n",
    "# scorer.get_perplexity(txt1,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.271672Z",
     "iopub.status.idle": "2025-01-27T15:50:20.271959Z",
     "shell.execute_reply": "2025-01-27T15:50:20.271853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ii=4\n",
    "# samples_new=samples.copy()\n",
    "# samples_new.loc[ii,\"text\"]=txt1\n",
    "# samples_new['score'] = samples_new['text'].map(lambda x: scorer.get_perplexity(x, batch_size=BATCH_SIZE))\n",
    "# for sh in range(6,10):\n",
    "#     df = get_best_plex(df=samples_new, ix=ii, shift=sh, rev=True, tf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.272662Z",
     "iopub.status.idle": "2025-01-27T15:50:20.272979Z",
     "shell.execute_reply": "2025-01-27T15:50:20.272868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def find_best_reorder(input_str, index):\n",
    "#     words = input_str.split()\n",
    "#     if index < 0 or index >= len(words):\n",
    "#         raise ValueError(\"Index is out of range\")\n",
    "\n",
    "#     best_sentence = input_str\n",
    "#     best_perplexity = scorer.get_perplexity(input_str, 4)\n",
    "#     #print(f\"Start perplexity: {best_perplexity}, for sentence: {input_str}\")\n",
    "\n",
    "#     word_to_move = words.pop(index)\n",
    "\n",
    "#     for i in range(len(words) + 1):\n",
    "#         #print(i)\n",
    "#         reordered_words = words[:]\n",
    "#         reordered_words.insert(i, word_to_move)\n",
    "#         reordered_sentence = ' '.join(reordered_words)\n",
    "\n",
    "#         #print(reordered_sentence)\n",
    "        \n",
    "#         perplexity = scorer.get_perplexity(reordered_sentence, 4)\n",
    "\n",
    "#         if perplexity < best_perplexity:\n",
    "#             print(f\"Checking permutation: {reordered_sentence} :with perplexity: {perplexity}\")\n",
    "\n",
    "# def optimize_sentence(input_str):\n",
    "#     words = input_str.split()\n",
    "#     best_sentence = input_str\n",
    "#     best_perplexity = scorer.get_perplexity(input_str, 4)\n",
    "#     print(f\"Initial perplexity: {best_perplexity}, for sentence: {input_str}\")\n",
    "\n",
    "#     for index in range(len(words)):\n",
    "#         print(f\"Index: {index}\")\n",
    "#         find_best_reorder(best_sentence, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.273769Z",
     "iopub.status.idle": "2025-01-27T15:50:20.274137Z",
     "shell.execute_reply": "2025-01-27T15:50:20.273981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ## I get a new good starting point\n",
    "# text_new = 'from and of to the as in that it we with not you have milk chocolate candy peppermint eggnog cookie fruitcake toy doll game puzzle greeting card wrapping paper bow candle wreath poinsettia snowglobe fireplace angel star wish dream night season wonder believe hope joy peace merry hohoho kaggle workshop'\n",
    "# optimize_sentence(text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.274810Z",
     "iopub.status.idle": "2025-01-27T15:50:20.275091Z",
     "shell.execute_reply": "2025-01-27T15:50:20.274974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# new_best_text = 'from and of to the as in that it we with not you have milk chocolate candy peppermint eggnog cookie fruitcake toy doll game puzzle greeting card wrapping paper bow candle fireplace wreath poinsettia snowglobe angel star wish dream night season wonder believe hope joy peace merry hohoho kaggle workshop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.275859Z",
     "iopub.status.idle": "2025-01-27T15:50:20.276153Z",
     "shell.execute_reply": "2025-01-27T15:50:20.276016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# samples.loc[4,\"text\"]=new_best_text\n",
    "# perplexities2 = []\n",
    "\n",
    "\n",
    "# for index, row in samples.iterrows(): # Step 1: Reorder the words based on POS tagging reordered_text = reorder_text(row[\"text\"])\n",
    "#     score=scorer.get_perplexity(row['text'],batch_size=BATCH_SIZE)\n",
    "#     perplexities2.append(score)\n",
    "#     print(f\"i={index}:{score}\")\n",
    "# np.sum(perplexities2)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-27T15:50:20.276866Z",
     "iopub.status.idle": "2025-01-27T15:50:20.277201Z",
     "shell.execute_reply": "2025-01-27T15:50:20.277038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# samples.to_csv(\"submission.csv\", index=False)a"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10229277,
     "sourceId": 88046,
     "sourceType": "competition"
    },
    {
     "datasetId": 4869119,
     "sourceId": 8215076,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6346030,
     "sourceId": 10589658,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10594224,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 735,
     "modelInstanceId": 3095,
     "sourceId": 4300,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 735,
     "modelInstanceId": 3102,
     "sourceId": 4307,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 3108,
     "modelInstanceId": 4761,
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 58218,
     "sourceId": 69768,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72255,
     "sourceId": 104492,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72260,
     "sourceId": 104587,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
